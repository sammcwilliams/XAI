=======
Anchors
=======

Usage
-----

Anchors are an approach that trains an explainer on your dataset, which then can be given an instance to interpret. The interpretation 
you are given is a series of if-then statements that describe the conditions for the prediction. In contrast to other explainability methods that take
individual instances for explanations - Anchors attempt to quanitfy the local area for which the explanation holds true, giving a figure for both precision and coverage.

.. code-block::

    xai.anchor.anchor_explanation(test_x[42], categorical_names={2:["Male","Female"]})


The key of the categorical_names parameter is the index of the feature, with the value being a list of all the possible values it can take.

Requisites
----------

The explainer requires the training data, the predict function of the model and an instance to explain.

Accepted Models/Data
--------------------

Data
^^^^

Currently supported datatypes are:

* Tabular data
* Image data
* Text data

As tabular data is the only one of these currently in scope, within this type the values are recognised as either:

* Numerical
* Categorical

Models
^^^^^^

As the explainer only requires the predict function to be passed as a parameter, in theory any model can be used.

Dimensionality
--------------

blah blah blah

Caveats
-------

When using Anchors you may find that you will struggle to find useful explanations for some complex problems, 
as the number of conditions required increases, or the precision/coverage decreases.

It is also possible for the same instance to give two different explanations. The creator of this technique says its unlikely however and can be mitigated by 
increasing the precision threshold.

You will find that in regression problems, the explainer will often spit out explanations for which it is 0% confident. This is due to the continuous nature of regression, however
this can be worked around by first transforming the targets into classes before training your model. Increasing the number of you classes you choose here will give you a greater 
prediction accuracy but will inevitably give you a less useful Anchor explanation, either because the confidence rating will drop, or more conditions will be used.

Also don't call this function more than once unless you **really** like ValueErrors. **<TODO>**

Output interpretation
---------------------

The function itself returns two variables. The first is the explanation object generated by the Anchor explainer, and the second uses this explanation
and interprets it in plain English.

This human-readable explanation looks as follows:

.. code-block::
    
    IF      petal length (cm) <= 4.30
    AND     petal width (cm) <= 0.30
    THEN PREDICT Class 1 WITH 100.0% CONFIDENCE.
    THIS EXPLANATION APPLIES TO 24.73% OF THE TRAINING DATA.

There is clearly a trade-off with precision and coverage with these explanations. You could choose to set the precision threshold lower and explain more instances
but less accurately, or you can attempt to explain what is specifically happening in a subset of the data.

The explanation object returned has a method ``exp.examples()`` which will return all instances in the training data for which the explanation applies. 
You can further specify whether you want to show only the falsely classified, or the correctly classified.
